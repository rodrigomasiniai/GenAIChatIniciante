## ChatDOCx: Seu Chatbot Personalizado com IA ü§ñ

### Um chatbot especialista em seus documentos, desenvolvido com Hugging Face e Python

- **Converse com seus documentos [fazendo perguntas](#perguntas-e-respostas) em um aplicativo web interativo.**
- [Resuma](#resumo-de-texto) seus documentos.
- Muitos [links ou contatos](#personalizacao)? Acesse-os facilmente.
- Crie seu pr√≥prio [dom√≠nio/t√≥pico](#dominios) personalizado para conversar, por exemplo, sobre erros de desenvolvimento que voc√™ gostaria de rastrear.
- Suporta a integra√ß√£o de diferentes [modelos de PNL](#recursos) do Hugging Face.
- **Gr√°tis para usar. `Sem necessidade de chaves de API`!**

**Veja um exemplo do chatbot respondendo a algumas perguntas:**

<div align="center">
    <img src="https://i.imgur.com/Tfv5J2j.gif" alt="ChatDOCx" width=""/>
</div>

> **Observa√ß√£o:** O ChatDOCx √© experimental e pode n√£o funcionar perfeitamente. Fa√ßa perguntas com contexto para obter melhores resultados. üòâ

---

## Conte√∫do

* [Teoria Simplificada](#teoria-simplificada)
    - [Transformers](#transformers)
    - [RoBERTa para Respostas a Perguntas](#roberta-para-respostas-a-perguntas)
    - [BART para Resumo de Texto](#bart-para-resumo-de-texto)
* [Executando o Chatbot](#executando-o-chatbot)
    - [Instalando as Depend√™ncias](#instalando-as-dependencias)
    - [Executando o Aplicativo Web](#executando-o-aplicativo-web)
* [Recursos](#recursos)
    - [Dom√≠nios](#dominios)
    - [Perguntas e Respostas](#perguntas-e-respostas)
    - [Resumo de Texto](#resumo-de-texto)
    - [Personaliza√ß√£o](#personalizacao)
    - [Tratamento de Erros](#tratamento-de-erros)
* [Pr√≥ximos Passos](#proximos-passos)
* [Refer√™ncias](#referencias)

---

## Teoria Simplificada

### Transformers

- **Transformers** s√£o arquiteturas de aprendizado profundo projetadas para processar dados sequenciais e revolucionaram as tarefas de PNL (Processamento de Linguagem Natural).
- Eles consistem em m√∫ltiplas camadas de `autoaten√ß√£o` (mecanismo que permite ponderar a import√¢ncia de diferentes tokens de entrada dinamicamente, capturando depend√™ncias de longo alcance nos dados) e `redes neurais feedforward`.
- Modelos pr√©-treinados como BERT, GPT, T5 etc., foram disponibilizados por grandes empresas, permitindo o aprendizado de transfer√™ncia para tarefas de PNL.

**Usaremos os seguintes modelos para o nosso chatbot:**

### RoBERTa para Respostas a Perguntas

O RoBERTa (`Robustly Optimized BERT Approach`) da Facebook AI √© uma melhoria do BERT (`Bidirectional Encoder Representations from Transformers`) do Google AI. Enquanto o BERT lan√ßou as bases para modelos baseados em transformers em PNL, o RoBERTa otimizou ainda mais o processo de pr√©-treinamento e alcan√ßou melhor desempenho ao aproveitar conjuntos de dados maiores e t√©cnicas de treinamento avan√ßadas:

- **T√©cnicas de Treinamento Avan√ßadas:** O RoBERTa incorporou t√©cnicas como `mascaramento din√¢mico` e `tamanhos de lote aumentados`. O mascaramento din√¢mico envolve mascarar tokens dinamicamente durante o pr√©-treinamento, permitindo que o modelo se concentre mais no aprendizado de informa√ß√µes contextuais. Al√©m disso, o RoBERTa usou mini-lotes maiores durante o treinamento, o que ajudou na melhor generaliza√ß√£o e otimiza√ß√£o.

- **Foco na Modelagem de Linguagem Mascarada (MLM):** Ao contr√°rio do BERT, que tamb√©m incluiu a tarefa de `previs√£o da pr√≥xima frase (NSP)` durante o pr√©-treinamento, o RoBERTa se concentrou apenas na tarefa de `MLM`. Ao dedicar todos os recursos para melhorar a precis√£o da previs√£o de tokens mascarados, o RoBERTa foi capaz de ajustar suas capacidades de compreens√£o da linguagem com mais efic√°cia.

### BART para Resumo de Texto

O BART (Bidirectional and Auto-Regressive Transformers) √© um modelo de sequ√™ncia para sequ√™ncia introduzido pela Facebook AI:

- **Bidirecional:** Ele pode processar sequ√™ncias de entrada nas dire√ß√µes direta e inversa. Essa capacidade bidirecional permite que o BART capture o contexto dos tokens anteriores e posteriores, aprimorando sua compreens√£o da sequ√™ncia de entrada.

- **Auto-Regressivo:** Ele emprega uma estrat√©gia de decodifica√ß√£o auto-regressiva durante a gera√ß√£o, onde gera um token por vez, da esquerda para a direita, com base nos tokens gerados anteriormente. Essa abordagem garante que cada token seja condicionado aos tokens gerados antes dele, permitindo que ele produza sa√≠das coerentes e contextualmente relevantes.

---

## Executando o Chatbot

### Instalando as Depend√™ncias

Primeiro, instale todas as depend√™ncias Python necess√°rias executando: ```pip install -r requirements.txt```

> **Observa√ß√£o:** O ambiente de desenvolvimento √© Windows/Python vers√£o 3.12.2 (pode haver conflitos de vers√£o entre as depend√™ncias, sistema operacional, hardware etc.).

### Executando o Aplicativo Web

O aplicativo web √© desenvolvido com Flask. Execute-o com: ```python nlp.py```. Na janela de comando, voc√™ dever√° ver algo como:

```
WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http:/100.0.1.0:8000
```

Este √© o endere√ßo do seu aplicativo web (copie e cole no navegador para acess√°-lo). Consulte a se√ß√£o [Recursos](#recursos) para personalizar o aplicativo.

---

## Recursos

O seguinte pode ser configurado em `mylib/config.json`:

```json
{
    "qa_model_name": "deepset/roberta-base-squad2",
    "summary_model_name": "facebook/bart-large-cnn",
    "app_name": "",
    "use_stopwords": true
}
```

- `qa_model_name` √© o modelo usado para respostas a perguntas (RoBERTa).
- `summary_model_name` √© o modelo usado para resumo de texto (BART).
- `app_name` √© opcional, caso queira exibir sua pr√≥pria marca no aplicativo.
- `use_stopwords` remover√° as palavras comuns em ingl√™s para que os modelos processem as perguntas com mais precis√£o.

### Dom√≠nios

Definir dom√≠nios/t√≥picos √© um componente essencial do chatbot, pois ele tem um desempenho melhor se os dados forem estruturados como um √∫nico dom√≠nio. Isso ajuda o bot a se lembrar do contexto por tr√°s dos dados ao responder a perguntas.

### Perguntas e Respostas

Ao executar o aplicativo, voc√™ precisa selecionar um dom√≠nio para come√ßar a fazer perguntas. Isso garante que voc√™ obtenha as melhores respostas poss√≠veis! üòâ Selecione um dom√≠nio, fa√ßa sua pergunta e pressione `Enter` ou clique em `Submit`, como mostrado no exemplo no in√≠cio.

### Resumo de Texto

Ap√≥s selecionar um dom√≠nio, basta incluir as palavras-chave `summary` ou `summarize` em sua pergunta, por exemplo, "summary of transformers" ou "summarize nlp", para obter um resumo do dom√≠nio:

<div align="center">
    <img src="https://i.imgur.com/KV90xe9.gif" alt="ChatDOCx" width=""/>
</div>

> **Observa√ß√£o:** O resumo de texto pode ser um pouco lento. O desempenho provavelmente depende do modelo, da quantidade de dados etc.

### Personaliza√ß√£o

O chatbot √© altamente personaliz√°vel, pois muitas fun√ß√µes s√£o projetadas do zero com flexibilidade. Por exemplo:

- **Adicionando dados:** Os dados s√£o armazenados na pasta `data` como arquivos `.txt`.
- **Dom√≠nios/t√≥picos:** s√£o exibidos automaticamente com base nos dados (nome do arquivo de texto). V√°rios dom√≠nios podem ser criados dependendo dos arquivos de texto na pasta `data`. A cor de um dom√≠nio tamb√©m pode ser alterada modificando o seguinte trecho de c√≥digo em `templates/ChatDOCx.html`:

```javascript
{% set topic_colors = {'Contacts': '#FFD700', 'Links': '#FFD700', 'Errors': '#f36262'} %}
```

- **Modelos:** os modelos usados s√£o testados/selecionados com base nas respostas que fornecem, mas diferentes modelos do Hugging Face podem ser usados (consulte `mylib/config.json` para configurar o modelo e a se√ß√£o [Refer√™ncias](#referencias) para obter uma lista de modelos do Hugging Face).
- **Aplicativo Web:** √© projetado do zero com estilo HTML/JavaScript e voc√™ pode personaliz√°-lo como desejar (consulte a pasta `templates`).
- **Links:** podem ser adicionados em `data/links.json`. Basta selecionar o dom√≠nio `Links` e incluir a palavra-chave `link` seguida de sua pergunta, por exemplo, "link to transformers".

> **Observa√ß√£o:** Os links tamb√©m s√£o correspondidos √†s suas perguntas. No exemplo abaixo, a palavra-chave `transformers` em sua pergunta tamb√©m est√° no banco de dados `links.json`, o que fornece uma resposta direta para `saiba mais` sobre transformers como um link clic√°vel.

- **Contatos:** podem ser adicionados em `data/contacts.json`. Basta selecionar o dom√≠nio `Contacts` e incluir a palavra-chave `contact` seguida de sua pergunta, por exemplo, "contact of huggingface".

<div align="center">
    <img src="https://i.imgur.com/MRqi30e.gif" alt="ChatDOCx" width=""/>
</div>

### Tratamento de Erros

Os erros ser√£o mostrados na barra vermelha no aplicativo. A l√≥gica implementada lida com a sele√ß√£o de dom√≠nio, entradas vazias/curtas e perguntas fora do escopo do chatbot para evitar desinforma√ß√£o. Abaixo est√° uma demonstra√ß√£o, incluindo um exemplo onde o bot lida com perguntas fora de seu conhecimento (**Observa√ß√£o:** `use_stopwords` aprimorar√° esta fun√ß√£o):

<div align="center">
    <img src="https://i.imgur.com/nEAShYw.gif" alt="ChatDOCx" width=""/>
</div>

---

## Pr√≥ximos Passos

- Implementar RAG (Retrieval-Augmented Generation) para aprimorar o conhecimento espec√≠fico do dom√≠nio.
- Fun√ß√£o de mem√≥ria para salvar o hist√≥rico do chat.
- Fun√ß√£o para lidar com v√°rios formatos de arquivo (atualmente, apenas arquivos .txt s√£o suportados) ou uma maneira melhor de buscar os dados em tempo real.

---

## Refer√™ncias

- Modelos de Perguntas e Respostas do Hugging Face: https://huggingface.co/models?pipeline_tag=question-answering&sort=trending
- Modelos de Resumo do Hugging Face: https://huggingface.co/models?pipeline_tag=summarization&sort=trending

---
